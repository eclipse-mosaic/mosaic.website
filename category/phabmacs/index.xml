<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PHABMACS | Eclipse MOSAIC â€“ A Multi-Domain and Multi-Scale Simulation Framework for Connected and Automated Mobility</title>
    <link>https://www.eclipse.dev/mosaic/category/phabmacs/</link>
      <atom:link href="https://www.eclipse.dev/mosaic/category/phabmacs/index.xml" rel="self" type="application/rss+xml" />
    <description>PHABMACS</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.eclipse.dev/mosaic/images/logo.svg</url>
      <title>PHABMACS</title>
      <link>https://www.eclipse.dev/mosaic/category/phabmacs/</link>
    </image>
    
    <item>
      <title>Evaluating Cooperative LiDAR Data Fusion for VRU Safety with MOSAIC Extended</title>
      <link>https://www.eclipse.dev/mosaic/post/cooperative-lidar-vru-safety/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://www.eclipse.dev/mosaic/post/cooperative-lidar-vru-safety/</guid>
      <description>&lt;p&gt;&lt;strong&gt;People traveling by bike, on foot or e-scooters are little protected in the event of a collision and therefore known as Vulnerable Road Users (VRUs). Modern sensor systems for automated driving such as LiDAR are able to detect VRUs, thus facilitate warnings and safety. Yet, in certain situations, local blind spots could occur. Data fusion from different vehicles could solve this issue - as it is shown with PHABMACS and MOSAIC Extended.&lt;/strong&gt;&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/header_hu96876ddb8b25fac64874ea85d049bd00_928574_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/header_hu96876ddb8b25fac64874ea85d049bd00_928574_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;70%&#34; height=&#34;795&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;During the research project &lt;strong&gt;




&lt;a href=&#34;https://reallab-hamburg.de/projekte/vernetzte-vulnerable-road-users/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RealLabor Hamburg&lt;/a&gt;&lt;/strong&gt;, we used MOSAIC to create a complex traffic situation for investigating the general benefit of LiDAR based object detection and the effectiveness of merging the LiDAR sensor data (point clouds) from multiple vehicles before applying object detection. In this case, the LiDAR data would not be processed locally on the vehicles but communicated to a server (possibly in the Mobile Edge Cloud) and processed there - accordingly it results in a Cooperative LiDAR Data Fusion.&lt;/p&gt;
&lt;h2 id=&#34;scenario-for-data-collection&#34;&gt;Scenario for Data Collection&lt;/h2&gt;
&lt;p&gt;As a first step, we set up a simulation scenario at the Sievekingplatz in Hamburg.
At the chosen road section, complicated traffic situations can arise due to the multi-lane road, the parallel bike lane, and the bus stop.
To simulate such a situation, we added a halting bus, a bike riding along the bike lane, and three passenger cars driving in a row.
With the &lt;strong&gt;





  
  

    

    
    
      
      
      
      
    

    
    

    
  

&lt;a href=&#34;https://www.eclipse.dev/mosaic/mosaic/docs/simulators/application_mapping/&#34;&gt;Mapping&lt;/a&gt;&lt;/strong&gt; configuration file, the vehicles are timed such that the second vehicle in the row can not perceive the bike when passing the bus, even in the safety-relevant close range.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-frame-of-the-sievekingplatz-scenario-the-trajectories-of-the-vehicles-are-marked-in-yellow&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/scenario_screenshot_huba8ebc700f8573edde0ea3a9a508d1cb_464271_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Frame of the Sievekingplatz scenario. The trajectories of the vehicles are marked in yellow.&#34;&gt;


  &lt;img data-src=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/scenario_screenshot_huba8ebc700f8573edde0ea3a9a508d1cb_464271_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;426&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Frame of the Sievekingplatz scenario. The trajectories of the vehicles are marked in yellow.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The simulation of LiDAR sensors is implemented in the &lt;strong&gt;PHABMACS&lt;/strong&gt; vehicle simulator, which is integrated in &lt;strong&gt;MOSAIC Extended&lt;/strong&gt;.
With the &lt;code&gt;phabmacs_config.json&lt;/code&gt; file it is possible to configure LiDAR sensors according to real products. In this case, we configured it according to the Velodyne HDL-64E.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;





  
  

    

    
    
      
      
      
      
    

    
    

    
  

&lt;a href=&#34;https://www.eclipse.dev/mosaic/mosaic/docs/simulators/application_simulator/&#34;&gt;Application Simulator&lt;/a&gt;&lt;/strong&gt;, each of the passenger cars is equipped with a LiDAR sensor which scans the environment and creates a 3D point cloud at a rate of 10 Hz.
To pool the LiDAR data from the passenger vehicles together at a server, we mapped a &lt;code&gt;LidarTransmissionApp&lt;/code&gt; on each vehicle, forwarding the LiDAR point cloud to the server, using the &lt;code&gt;PointCloudMessages&lt;/code&gt;.
Additionally, all simulated vehicles (cars, bus and bike) are mapped with a &lt;code&gt;ReportingApp&lt;/code&gt;, using the &lt;code&gt;ReportMessages&lt;/code&gt; to report vehicle information such as e.g., position and heading. These data are the ground truth for original location information, used in the later accuracy comparison.
On the server, a &lt;code&gt;DataManagementApp&lt;/code&gt; handles all received messages for the cooperative data fusion and exports data to log files, using the &lt;strong&gt;





  
  

    

    
    
      
      
      
      
    

    
    

    
  

&lt;a href=&#34;https://www.eclipse.dev/mosaic/mosaic/docs/visualization/filevis/&#34;&gt;Output Generator&lt;/a&gt;&lt;/strong&gt;.
The &lt;strong&gt;





  
  

    

    
    
      
      
      
      
    

    
    

    
  

&lt;a href=&#34;https://www.eclipse.dev/mosaic/mosaic/docs/simulators/network_simulator_cell/&#34;&gt;Cell Simulator&lt;/a&gt;&lt;/strong&gt; models the message transmission between the mobile entities and the server. An overview of all components can be seen below.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-scenario-components-overview&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/scenario_overview_hude024dadbf4b3a5d490057146ad8304b_45033_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Scenario components overview&#34;&gt;


  &lt;img data-src=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/scenario_overview_hude024dadbf4b3a5d490057146ad8304b_45033_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;458&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Scenario components overview
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;evaluation-of-lidar-data-processing&#34;&gt;Evaluation of LiDAR Data Processing&lt;/h2&gt;
&lt;p&gt;Based on the data collected from the simulation, we evaluated the effectiveness of merging sensor data before object detection.
We specifically focused on the detectability of the bike based on the point cloud data.
For this purpose, we estimated object detection results by filtering the LiDAR hits on the bike, using locational vehicle data gained from the &lt;code&gt;ReportMessages&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In FIGURE 3, the number of LiDAR hits on the bike at each sampling step are displayed.
As there are always some LiDAR hits, we can conclude that at all times the bike is visible from at least one perspective.
Though at certain frames some passenger cars could not perceive the bike, as it was occluded by the bus from their perspective.
Especially when &lt;code&gt;car_2&lt;/code&gt; is passing the bike, there are some frames (~16.8-17.8s) at which the general visibility of the bike is very limited.
In these frames an object detection could profit a lot from merging the individual point clouds, as it would increase the overall amount of LiDAR hits and therefore the detection of an object could be more likely.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-total-amount-of-lidar-hits-on-the-car-from-each-lidar-and-the-sum-of-all-lidar-hits&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/LidarHits_huf4cfe39e38ce308d65f0c0bd8a082301_71957_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The total amount of LiDAR hits on the car from each LiDAR and the sum of all LiDAR hits.&#34;&gt;


  &lt;img data-src=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/LidarHits_huf4cfe39e38ce308d65f0c0bd8a082301_71957_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;46%&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The total amount of LiDAR hits on the car from each LiDAR and the sum of all LiDAR hits.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;To further evaluate the merging of point clouds we compared it with the merging of objects.
For the &lt;em&gt;PointCloudMerge&lt;/em&gt;, the latest point cloud from all vehicles with a LiDAR sensor are unified, resulting in one large global point cloud. This point cloud is then used to derive a bounding box for the bike.
For the &lt;em&gt;ObjectMerge&lt;/em&gt; a bounding box for each individual point cloud is derived. From all these bounding boxes, the one which is dimensionally most similar to the actual bounding box of the bike, is selected as final result.
To compare the results of both processing methods the &lt;em&gt;Bounding Box RMSE&lt;/em&gt; was used as a metric.
This metric represents the relation between the bounding box calculated from the point clouds and the actual bounding box of the bike, considering width and length of the bounding boxes.
A value of &lt;code&gt;0.0&lt;/code&gt; corresponds to no error and therefore a perfect match of the bounding boxes.&lt;/p&gt;
&lt;p&gt;In FIGURE 4 the results of both merging methods can be seen.
The results show that the bounding boxes are more accurate in many time steps when applying the &lt;em&gt;PointCloudMerge&lt;/em&gt; method.
Also, some peaks in the metric observable with the &lt;em&gt;ObjectMerge&lt;/em&gt; method could be averted.
The often more accurate bounding boxes with &lt;em&gt;PointCloudMerge&lt;/em&gt; could lead to an overall more stable object detection and more accurate labeling, which is important to correctly estimate behavior of objects in an autonomous driving context.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure--comparison-of-different-data-merging-methods-with-the-boundingbox-rmse-metric&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/PointCloudMerge_vs_ObjectMerge_hue0c3c11dd13aa5205121fc5b5b78d64a_63185_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;: Comparison of different data merging methods with the BoundingBox RMSE metric.&#34;&gt;


  &lt;img data-src=&#34;https://www.eclipse.dev/mosaic/mosaic/post/cooperative-lidar-vru-safety/PointCloudMerge_vs_ObjectMerge_hue0c3c11dd13aa5205121fc5b5b78d64a_63185_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;46%&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    : Comparison of different data merging methods with the &lt;em&gt;BoundingBox RMSE&lt;/em&gt; metric.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We created a complex traffic situation collecting LiDAR sensor data for the preliminary evaluation of a system for merging 3D point clouds before applying object detection.&lt;/p&gt;
&lt;p&gt;MOSAICs &lt;strong&gt;Multi-Domain&lt;/strong&gt; aspect was utilized as the evaluated system including moving vehicles, a bus and a bike, dedicated applications for reporting and data processing, as well as the communication link for data transmission was modelled with the simulators PHABMACS, Application Simulator and Cell Simulator. Especially PHABMACS can model high-resolution LiDAR sensors and precise driving maneuvers - just according to MOSAICs &lt;strong&gt;Multi-Scale&lt;/strong&gt; aspect. On top, the Output Generator supported with formatted data recording for post-processing of simulation results.&lt;/p&gt;
&lt;p&gt;The results show that merging point clouds is indeed a processing method for LiDAR data worth to investigate further, as it could increase the likelihood of detecting an object and increase the accuracy of the labeling.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Further information about RealLabHH and the use case of protecting VRUs: &lt;a href=&#34;https://reallab-hamburg.de/projekte/vernetzte-vulnerable-road-users/&#34;&gt;https://reallab-hamburg.de/projekte/vernetzte-vulnerable-road-users/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RealLabHH was completed in December 2021 after 1.5 years. It was funded by the German Federal Ministry of Transport and Digital Infrastructure (BMVI).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing Remote-Operated Driving virtually with Eclipse MOSAIC</title>
      <link>https://www.eclipse.dev/mosaic/post/remote-operated-driving/</link>
      <pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://www.eclipse.dev/mosaic/post/remote-operated-driving/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;overview.png&#34; alt=&#34;alternative text for search engines&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remote-Operated Driving&lt;/strong&gt; is the bridge technology from human towards fully automated driving.
In situations outside the driving domain of a highly-automated vehicle, e.g. if data is missing, or the autonomous function is unsure to
make a certain decision, remote-operation is the key. Also, in other use-cases, remote-operated driving is a helpful
technique, e.g. for driving electric car-sharing vehicles to their charging stations, or maneuvering vehicles remotely through
a parking garage.&lt;/p&gt;
&lt;p&gt;In all those use-cases, a human operator would &amp;ldquo;steer&amp;rdquo; the vehicle remotely. All sensor information would be sent over the 5G network
to the operator who can then decide on the action or trajectory the vehicle should follow. The information the operator
receives could be any sensor data from the vehicle, such as camera data, LiDAR data,
or already compiled information like detected objects and free space.
With Mobile Edge Computing and sensor fusion, the information could be enriched by other vehicles or stationary sensors.&lt;/p&gt;
&lt;p&gt;Virtual Testing with MOSAIC helps to dive deeper into this topic. This study bases on LiDAR data for presentation of the operator view,
which allows selecting different viewing angles as well as sensor fusion of different perspectives from other vehicles for a
holistic environment model. The final result can be seen in the video below.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/KC5ZTy1CDz8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;a title=&#34;Watch &#39;Simulating Remote Operated Driving | FOKUS Smart Mobility&#39; on YouTube&#34; href=&#34;https://www.youtube.com/watch?v=KC5ZTy1CDz8&#34;&gt;
&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; Watch &amp;ldquo;Simulating Remote Operated Driving | FOKUS Smart Mobility&amp;rdquo; on YouTube
&lt;/a&gt;&lt;/p&gt;
&lt;br/&gt;
&lt;p&gt;Eclipse MOSAIC has been used to couple the vehicle simulator PHABMACS with the MOSAIC Application
simulator, in which a custom application has been deployed providing the operator view.
The vehicle simulator PHABMACS is responsible for vehicle dynamics and sensor data, in this case LiDAR data.
The message exchange of LiDAR as well as vehicle control data has been simulated by integrating the MOSAIC Cell simulator.
In this way, we could analyze the influence of communication properties, e.g. latencies and
different connection qualities such as low capacities or packet losses, on the application.
For the hybrid test setup with virtual world and real application for the human operators,
the whole simulation has to run in real time, which is possible with Eclipse MOSAIC (see parameter &lt;code&gt;--realtime-brake 1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;overview_mosaic.png&#34; alt=&#34;Tele-Operated Driving Simulation with Eclipse MOSAIC&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing mobility scenarios with the Open-Source simulation environment Eclipse MOSAIC</title>
      <link>https://www.eclipse.dev/mosaic/post/eclipse-mosaic/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://www.eclipse.dev/mosaic/post/eclipse-mosaic/</guid>
      <description>&lt;p&gt;&lt;strong&gt;On the occasion of EclipseCon 2020, Fraunhofer FOKUS launches its simulation environment Eclipse MOSAIC. This solution is based on VSimRTI (Vehicle-2-X Simulation Runtime Infrastructure), which has been developed over the last 15 years in close cooperation with the DCAITI of the TU Berlin and has already been used by more than 600 partners to test mobility services and traffic scenarios. Eclipse MOSAIC is now partially available as open-source.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whether dynamic lane assignment or traffic light phase assistant, new mobility services are designed to increase safety, efficiency, comfort, and facilitate environmentally friendly transport. The Eclipse MOSAIC simulation environment allows to explore how this can be achieved, before the services are tested in field trials on the road. Eclipse MOSAIC can also be used for testing driver assistance systems and to optimize the entire traffic.&lt;/p&gt;
&lt;h2 id=&#34;flexible-coupling-of-simulators&#34;&gt;Flexible coupling of simulators&lt;/h2&gt;
&lt;p&gt;Eclipse MOSAIC integrates, depending on the simulation scenario, different aspects like individual building blocks into a holistic system, e.g., traffic congestion, battery charging of electric cars, or communication between other road users and a central cloud. The level of detail for individual aspects is variable: from a rough mobility scenario for an entire city to detailed individual driving maneuvers.&lt;/p&gt;
&lt;p&gt;The open-source version of Eclipse MOSAIC already includes several simulators, e.g., Eclipse SUMO for traffic and OMNeT++ and ns-3 for communication. Further simulators can be coupled, e.g., Fraunhofer FOKUS offers the simulator PHABMACS for the realistic modeling of autonomous vehicles.&lt;/p&gt;
&lt;p&gt;In addition to the simulator coupling, Eclipse MOSAIC manages the following tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Federation: Individual simulators are interchangeable within a scenario.&lt;/li&gt;
&lt;li&gt;Interaction: Information from one simulator is also taken into account by others.&lt;/li&gt;
&lt;li&gt;Time: All simulators run synchronously.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, Eclipse MOSAIC offers several tools for evaluation and visualization of the results, which are also included in the open-source package.&lt;/p&gt;
&lt;p&gt;In the recently completed EU project INFRAMIX, Eclipse MOSAIC was used to test scenarios for the future road that allow mixed traffic between conventional and automated vehicles.&lt;/p&gt;
&lt;p&gt;Fraunhofer FOKUS has been a strategic development member of the Eclipse Foundation since May of this year and works in close cooperation with the partners of the working groups OpenMobility and openADx (Open Source for Autonomous Driving).&lt;/p&gt;
&lt;p&gt;Further information about Eclipse MOSAIC:
&lt;a href=&#34;https://www.eclipse.dev/mosaic&#34;&gt;https://www.eclipse.dev/mosaic&lt;/a&gt;
&lt;a href=&#34;https://github.com/eclipse-mosaic/mosaic&#34;&gt;https://github.com/eclipse-mosaic/mosaic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Further information about INFRAMIX:
&lt;a href=&#34;https://www.fokus.fraunhofer.de/de/fokus/news/inframix-projekt_2020_08&#34;&gt;https://www.fokus.fraunhofer.de/de/fokus/news/inframix-projekt_2020_08&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Further information about EclipseCon:
&lt;a href=&#34;https://www.eclipsecon.org/2020&#34;&gt;https://www.eclipsecon.org/2020&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
